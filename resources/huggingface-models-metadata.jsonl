{"id": "BISPL-KAIST/Text-Guided-DiffusionDet-CXR", "model_id": "Text-Guided-DiffusionDet-CXR", "tasks": ["cxr-image-manipulation"], "endpoint": "https://qv9hl2x37dp4o6g9.us-east-1.aws.endpoints.huggingface.cloud", "model_card": "Model Card: DiffusionDet-cxr\n\t\n\t\tModel Details\n\t\nModel Name : DiffusionDet-cxr\nModel Type : Diffusion Based Object Detection Model\nFramework : PyTorch\n\t\tOverview\n\t\nDiffusionDet-cxr is an object detection model designed for CXR (Chest X-Ray) datasets, based on the Diffusion Based object detection model architecture. It specializes in identifying and localizing relevant information within textual data by drawing bounding boxes around the most pertinent sections of a given text.\n\t\tTraining Data\n\t\nDataset : VinBig Dataset\nTotal 'Abnormal' Data : 4394 instances (#instances: 23955)\nTrain Data : 3296 instances (#instances: 18024)\nValidation Data : 1098 instances (#instances: 5931)\nClass Number : 14\nClass Names : ['Aortic_enlargement', 'Atelectasis', 'Calcification', 'Cardiomegaly', 'Consolidation', 'ILD', 'Nodule/Mass', 'Other_lesion', 'Pleural_effusion', 'Pleural_thickening', 'Pneumothorax', 'Pulmonary_fibrosis']\n\t\tIntended Use\n\t\n\t\tApplications\n\t\nText Processing : Highlighting relevant sections in text data, such as medical reports or articles.\nMedical Reports : Future development aims to integrate with medical report analysis to highlight sections relevant to specific medical conditions.\n\t\tLimitations\n\t\nDataset Bias : The model's performance may be influenced by the characteristics and distribution of the VinBig dataset.\nGeneralization : Effectiveness can vary based on the diversity and complexity of textual inputs."}
{"id": "BISPL-KAIST/Text-Guided-DiffusionDet-Endoscope", "model_id": "Text-Guided-DiffusionDet-Endoscope", "tasks": ["endoscope-image-manipulation"], "endpoint": "https://liljsfc500oej5fm.us-east-1.aws.endpoints.huggingface.cloud", "model_card": "Model Card: DiffusionDet-endoscope\n\t\n\t\tModel Details\n\t\nModel Name: DiffusionDet-endoscope\nModel Type: Diffusion Based Object Detection Model\nFramework: PyTorch\n\t\tOverview\n\t\nDiffusionDet-endoscope is an object detection model designed for endoscopic datasets, based on the Diffusion Based object detection model architecture. It specializes in identifying and localizing relevant information within textual data by drawing bounding boxes around the most pertinent sections of a given text.\n\t\tTraining Data\n\t\nDataset: Open Source Endoscope Dataset\nTotal 'Abnormal' Data: 3500 instances (#instances: 3938)\nTrain Data: 2859 instances (#instances: 3213)\nValidation Data: 641 instances (#instances: 725)\nClass Number: 1\nClass Name: ['This is Polyp.']\n\t\tIntended Use\n\t\n\t\tApplications\n\t\nText Processing: Highlighting relevant sections in text data, such as medical reports or articles.\nMedical Reports: Future development aims to integrate with medical report analysis to highlight sections relevant to specific medical conditions, particularly those related to endoscopic findings.\n\t\tLimitations\n\t\nDataset Bias: The model's performance may be influenced by the characteristics and distribution of the Open Source Endoscope dataset.\nGeneralization: Effectiveness can vary based on the diversity and complexity of textual inputs."}
{"id": "BISPL-KAIST/LDMol", "model_id": "LDMol", "tasks": ["text-to-molecule"], "endpoint": "", "model_card": "LDMol\n\t\nOfficial GitHub repository for LDMol, a latent text-to-molecule diffusion model.\nThe details can be found in the following paper: \nLDMol: Text-Conditioned Molecule Diffusion Model Leveraging Chemically Informative Latent Space. (arxiv 2024)\nLDMol not only can generate molecules according to the given text prompt, but it's also able to perform various downstream tasks including molecule-to-text retrieval and text-guided molecule editing.\nThe model checkpoint and data are too heavy to be included in this repo and can be found in here.\n\t\tRequirements\n\t\nRun conda env create -f requirements.yaml and it will generate a conda environment named ldmol.\n\t\tInference\n\t\nCheck out the arguments in the script files to see more details.\n1. text-to-molecule generation\nzero-shot: The model gets a hand-written text prompt.CUDA_VISIBLE_DEVICES=0,1 torchrun --nnodes=1 --nproc_per_node=2 inference_demo.py --num-samples 100 --ckpt ./Pretrain/checkpoint_ldmol.pt --prompt=\"This molecule includes benzoyl group.\" --cfg-scale=5\nbenchmark dataset: The model performs text-to-molecule generation on ChEBI-20 test set. The evaluation metrics will be printed at the end.TOKENIZERS_PARALLELISM=false CUDA_VISIBLE_DEVICES=0 torchrun --nnodes=1 --nproc_per_node=1 inference_t2m.py --ckpt ./Pretrain/checkpoint_ldmol_chebi20.pt --cfg-scale=3.5\n2. molecule-to-text retrieval\nThe model performs molecule-to-text retrieval on the given dataset. --level controls the quality of the query text(paragraph/sentence). --n-iter is the number of function evaluations of our model.\nTOKENIZERS_PARALLELISM=false CUDA_VISIBLE_DEVICES=0 torchrun --nnodes=1 --nproc_per_node=1 inference_retrieval_m2t.py --ckpt ./Pretrain/checkpoint_ldmol.pt --dataset=\"./data/PCdes/test.txt\" --level=\"paragraph\" --n-iter=10\n3. text-guided molecule editing\nThe model performs a DDS-style text-guided molecule editing. --source-text should describe the --input-smiles. --target-text is your desired molecule description.\nTOKENIZERS_PARALLELISM=false CUDA_VISIBLE_DEVICES=0 torchrun --nnodes=1 --nproc_per_node=1 inference_dds.py --ckpt ./Pretrain/checkpoint_ldmol.pt --input-smiles=\"C[C@H](CCc1ccccc1)Nc1ccc(C#N)cc1F\" --source-text=\"This molecule contains fluorine.\" --target-text=\"This molecule contains bromine.\"\n\t\tAcknowledgement\n\t\nThe code for DiT diffusion model is based on & modified from the official code of DiT.\nThe code for BERT with cross-attention layers xbert.py and schedulers are modified from the one in ALBEF."}
{"id": "BISPL-KAIST/llm-cxr", "model_id": "llm-cxr", "tasks": ["cxr-to-report-generation", "report-to-cxr-generation", "cxr-visual-qestion-answering"], "inputs": {"instruction": "This is a text describing the task the model needs to perform. For example, 'Generate a chest X-ray image that corresponds to the entered free-text radiology report' is an instruction to generate a chest X-ray image based on the provided text report. 'What is the size of the pleural effusions?' is an instruction to answer a question about the given chest X-ray image.", "input": "This is the data the model will process. If a text report is given, it will contain the content of the report. If a chest X-ray image is given, it will contain the URL of the image. For example, if a text report is provided, it might look like 'Bilateral, diffuse, confluent pulmonary opacities. Differential diagnoses include severe pulmonary edema ARDS or hemorrhage.' If an image is provided, it might look like 'https://chatmedi-s3.s3.ap-northeast-2.amazonaws.com/your-image-file.png'."}, "endpoint": "https://xpu4mkwtcops4gr4.us-east-1.aws.endpoints.huggingface.cloud", "model_card": "LLM-CXR: Instruction-Finetuned LLM for CXR Image Understanding and Generation\n\t\nThis repository is the official implementation of the paper LLM-CXR: Instruction-Finetuned LLM for CXR Image Understanding and Generation (arxiv).\nLLM-CXR utilizes a VQ-GAN that can tokenize CXR images into a sequence of image tokens, and has the ability to understand and generate these image tokens with its text tokens. \nThe endpoint of LLM-CXR supports three different inference scenarios using LLM-CXR.\nText-only Q&A, which can also be done with vanilla LLMs.\nInput image comprehension for CXR report writing and CXR question answering.\nImage output generation for CXR generation with a given report.\n\t\tGeneration example\n\t\nFor more generation examples, see the paper on arxiv.\n\t\tReport-to-CXR\n\t\n\t\tCXR-to-report\n\t\n\t\tCXR VQA"}
{"id": "BISPL-KAIST/spmm", "model_id": "spmm", "tasks": ["text-to-molecule", "molecule-to-text"], "endpoint": "https://ls89aq06xb80myza.us-east-1.aws.endpoints.huggingface.cloud", "model_card": "Model Card: SPMM - Structure-Property Multi-Modal learning for molecules\n\t\n\t\tModel Details\n\t\nModel Name: Structure-Property Multimodal Model (SPMM)\nVersion: 1.0Model Type: Pre-trained transformer\nFramework: PyTorch\n\t\tOverview\n\t\nThe official GitHub for SPMM, a multi-modal molecular pre-trained model for a synergistic comprehension of molecular structure and properties.\nSPMM is pre-trained with a large, paired dataset of molecular structure and its chemical properties.\nMolecule structure will be given in SMILES, and we used 53 simple chemical properties to build a property vector(PV) of a molecule.\nThe current SPMM endpoint supports two services: the PV-to-SMILES generation task and the SMILES-to-PV generation task.\nPV-to-SMILES generation gets an input PV and generates SMILES that agree with the given PV.\nSMILES-to-PV generation gets a SMILES string and predicts 53 property elements in its PV.\nThe details can be found in the following paper:\nBidirectional Generation of Structure and Properties Through a Single Molecular Foundation Model. (Nature Communications 2024)(arxiv)\n\t\tFiles\n\t\ndata/: Contains the data used for the experiments in the paper. (you have to make this folder and put the data that you downloaded from the link above.)\nPretrain/: Contains the checkpoint of the pre-trained SPMM. (you have to make this folder and put the checkpoint that you downloaded from the link above.)\nvocab_bpe_300.txt: Contains the SMILES tokens for the SMILES tokenizer.\nproperty_name.txt: Contains the name of the 53 chemical properties.\nnormalize.pkl: Contains the mean and standard deviation of the 53 chemical properties that we used for PV.\ncalc_property.py: Contains the code to calculate the 53 chemical properties and build a PV for a given SMILES. Modify this code accordingly to utilize SPMM pre-training for your custom PVs.\nSPMM_models.py: Contains the code for the SPMM model and its pre-training codes.\nSPMM_pretrain.py: runs SPMM pre-training.\nd_*.py: Codes for the downstream tasks.\n\t\tRequirements\n\t\nRun pip install -r requirements.txt to install the required packages.\n\t\tCode running\n\t\nArguments can be passed with commands, or be edited manually in the running code.\nPV-to-SMILES generation\nbatched: The model takes PVs from the molecules in input_file, and generates molecules with those PVs using k-beam search. The generated molecules will be written in generated_molecules.txt.python d_pv2smiles_batched.py --checkpoint './Pretrain/checkpoint_SPMM.ckpt' --input_file './data/pubchem_1k_unseen.txt' --k 2\nsingle: The model takes one query PV and generates n_generate molecules with that PV using k-beam search. The generated molecules will be written in generated_molecules.txt. Here, you need to build your input PV in the code. Check the four examples that we included.python d_pv2smiles_single.py --checkpoint './Pretrain/checkpoint_SPMM.ckpt' --n_generate 1000 --stochastic True --k 2\nSMILES-to-PV generation\nThe model takes the query molecules in input_file, and generates their PV.\npython d_smiles2pv.py --checkpoint './Pretrain/checkpoint_SPMM.ckpt' --input_file './data/pubchem_1k_unseen.txt'\n\t\tAcknowledgement\n\t\nThe code for BERT with cross-attention layers xbert.py and schedulers are modified from the one in ALBEF.\nThe code for SMILES augmentation is taken from pysmilesutils."}
{"id": "BISPL-KAIST/Text-Guided-DiffusionDet-CT", "model_id": "Text-Guided-DiffusionDet-CT", "tasks": ["ct-image-manipulation"], "endpoint": "", "model_card": ""}
{"id": "BISPL-KAIST/DDS", "model_id": "DDS", "tasks": ["ct-reconstruction"], "endpoint": "https://ofrbdkjh8xouep7r.us-east-1.aws.endpoints.huggingface.cloud", "model_card": "Model Card: Decomposed Diffusion Sampler (DDS)\n\t\n\t\tModel Details\n\t\nModel Name: Decomposed Diffusion Sampler (DDS)\nVersion: 1.0Model Type: Unconditional diffusion model\nFramework: PyTorch\n\t\tOverview\n\t\nThe underlying model in this framework is an unconditional model trained on AAPM CT data, aiming at capturing the prior data distribution of the high-quality CT images. In this framework,\nthe unconditional diffusion model will be used as a diffusion model-based inverse problem solver (DIS), which is effective for reconstructing CT in sparse-view or limited-angle situations.\n\t\tTraining Data\n\t\nDataset: AAPM (American Association of Physicists in Medicine) CT Data  \nData Size: Approximately 3000 2D slices of size 256 x 256 \nData Description: The AAPM dataset contains high-resolution CT scans collected from various clinical cases, including both normal and pathological conditions. The data has been anonymized to protect patient privacy.\n\t\tIntended Use\n\t\n\t\tApplications\n\t\nMedical Imaging: Reducing radiation dose in CT scans by enabling high-quality reconstructions from sparse-view and limited-angle projections.\nResearch: Serving as a baseline for further research in the field of CT medical image reconstruction.\n\t\tLimitations\n\t\nData Bias: The model is trained on AAPM data and may not generalize sufficiently well to CT scans from different sources or modalities.\nComputationally Intensive: Reconstruction may take about 10 seconds on a A10 GPU, which is more time-consuming than the typical one-step feed-forward neural network reconstruction, due to the iterative nature of the diffusion process.\n\t\tEthical Considerations\n\t\nClinical Validation: The model should undergo rigorous clinical validation before being used in a real-world clinical setting to ensure safety and efficacy."}
{"id": "danyalmalik/stable-diffusion-chest-xray", "model_id": "stable-diffusion-chest-xray", "tasks": ["report-to-cxr-generation"], "inputs": "text", "endpoint": "https://xac5q8gn5qpt6sfc.us-east-1.aws.endpoints.huggingface.cloud", "model_card": "Chest-Xray-Fine-Tuning---trained-on-chest-xray14-dataset-(only-1k-images-as-of-now) Dreambooth model trained by danyalmalik with TheLastBen's fast-DreamBooth notebook\nTest the concept via A1111 Colab fast-Colab-A1111"}
{"id": "openai/GPT", "model_id": "GPT", "tasks": ["question-answering-about-medical-domain", "summarization"], "inputs": "", "endpoint": "", "model_card": "Can answer routine questions and simple, non-specialized questions about healthcare. You should choose this model when none of the other models are suitable to fulfill the user's commands."}
{"id": "starmpcc/Asclepius-Llama3-8B", "model_id": "Asclepius-Llama3-8B", "tasks": ["clinical-note-analysis"], "inputs": {"note": "A summary of the patient's medical details during their hospital stay (e.g., diagnosis, treatment).", "question": "A specific question about the discharge summary (e.g., diagnosis, follow-up instructions)."}, "endpoint": "https://pabu7y41c5r1juux.us-east-1.aws.endpoints.huggingface.cloud", "model_card": "This is an official model checkpoint for Asclepius-Llama3-8B [(arxiv)](https://arxiv.org/abs/2309.00237). This model is an enhanced version of Asclepius-7B, by replacing the base model with Llama-3 and increasing the max sequence length to 8192. ## UPDATE ### 2024.01.10 - Asclepius-R, the variant of Asclepius that trained on MIMIC-III discharge summaries, is now available on [Physionet](https://physionet.org/content/asclepius-r/1.0.0/)! ## Model Details ### Model Description <!-- Provide a longer summary of what this model is. --> - **Model type:** Clinical LLM (Large Language Model) - **Language(s) (NLP):** English - **License:** CC-BY-NC-SA 4.0 - **Finetuned from model [optional]:** Llama3-8B **Repository:** https://github.com/starmpcc/Asclepius - **Paper:** https://arxiv.org/abs/2309.00237 - **Data:** https://huggingface.co/datasets/starmpcc/Asclepius-Synthetic-Clinical-Notes ## Uses This model can perform below 8 clinical NLP tasks, with clincal notes. - Named Entity Recognition - Abbreviation Expansion - Relation Extraction - Temporal Information Extraction - Coreference Resolution - Paraphrasing - Summarization - Question Answering ### Out-of-Scope Use  ONLY USE THIS MODEL FOR RESEARCH PURPOSE!! ## How to Get Started with the Model ```python prompt = \"You are an intelligent clinical languge model. Below is a snippet of patient's discharge summary and a following instruction from healthcare professional. Write a response that appropriately completes the instruction. The response should provide the accurate answer to the instruction, while being concise. [Discharge Summary Begin] {note} [Discharge Summary End] [Instruction Begin] {question} [Instruction End] \" from transformers import AutoTokenizer, AutoModelForCausalLM tokenizer = AutoTokenizer.from_pretrained(\"starmpcc/Asclepius-Llama3-8B\", use_fast=False) model = AutoModelForCausalLM.from_pretrained(\"starmpcc/Asclepius-Llama3-8B\") note = \"This is a sample note\" question = \"What is the diagnosis?\" model_input = prompt.format(note=note, question=question) input_ids = tokenizer(model_input, return_tensors=\"pt\").input_ids output = model.generate(input_ids) print(tokenizer.decode(output[0])) ``` ## Training Details ### Training Data https://huggingface.co/datasets/starmpcc/Asclepius-Synthetic-Clinical-Notes ### Training Procedure - Initial training was conducted using causal language modeling on synthetic clinical notes. - It was then fine-tuned with clinical instruction-response pairs. - For a comprehensive overview of our methods, our upcoming paper will serve as a resource. #### Training Hyperparameters - We followed config used in [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) - #### Speeds, Sizes, Times - Pre-Training (1 epoch): 2h 59m with 4x A100 80G - Instruction Fine-Tuning (3 epoch): 30h 41m with 4x A100 80G"}